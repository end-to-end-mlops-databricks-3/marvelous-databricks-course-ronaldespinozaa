## üéì **Learning Path & Notebooks**

This project demonstrates a **progressive learning approach** through structured notebooks:

### **üìö Week-by-Week Progression**

#### **Week 1: Foundation (`week1.py`)**
- ‚úÖ Environment setup and validation
- ‚úÖ Databricks connectivity testing
- ‚úÖ MLflow experiment initialization
- ‚úÖ Data loading and basic exploration

#### **Week 2: Core MLOps (`week2_*.py`)**
- ‚úÖ **Basic Model**: Standard ML pipeline with preprocessing
- ‚úÖ **Custom Model**: Advanced pyfunc wrapper with business logic
- ‚úÖ **Model Registry**: Unity Catalog integration and versioning
- ‚úÖ **Experiment Tracking**: Comprehensive MLflow logging

#### **Week 3: Advanced Feature Engineering (`week3_*.py`)**
- ‚úÖ **Feature Store Demo**: Interactive exploration of FE concepts
- ‚úÖ **Production Training**: Complete FE pipeline with infrastructure
- ‚úÖ **Advanced Features**: Risk scoring, campaign analysis, client profiling
- ‚úÖ **Enterprise Patterns**: Feature lineage, optimization, governance

### **üéØ Notebook Usage Guide**

| Notebook | Purpose | When to Use | Duration |
|----------|---------|-------------|----------|
| `week1.py` | Setup validation | First time setup | 15 min |
| `week2_train_basic_model.py` | Learn core patterns | After data processing | 20 min |
| `week2.train_register_custom_model.py` | Advanced modeling | After basic model | 25 min |
| `week3_feature_engineering_demo.py` | FE exploration | Learn FE concepts | 45 min |
| `week3_train_register_fe_model.py` | Production FE | Complete FE training | 30 min |# üè¶ Bank Marketing MLOps Project

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/release/python-3110/)
[![Databricks](https://img.shields.io/badge/platform-Databricks-red.svg)](https://databricks.com/)
[![MLflow](https://img.shields.io/badge/MLflow-2.17.0-green.svg)](https://mlflow.org/)
[![Delta Lake](https://img.shields.io/badge/Delta%20Lake-3.3.0-orange.svg)](https://delta.io/)

> **End-to-End MLOps Implementation for Bank Marketing Campaign Prediction**

A comprehensive MLOps project demonstrating production-ready machine learning workflows for predicting bank marketing campaign success using Databricks, MLflow, and advanced Feature Engineering patterns.

---

## üìã **Table of Contents**

- [üéØ Project Overview](#-project-overview)
- [üèóÔ∏è Architecture](#Ô∏è-architecture)
- [üìÅ Project Structure](#-project-structure)
- [üöÄ Getting Started](#-getting-started)
- [‚öôÔ∏è Configuration](#Ô∏è-configuration)
- [üìä Data Pipeline](#-data-pipeline)
- [ü§ñ Model Training](#-model-training)
- [üè™ Feature Engineering](#-feature-engineering)
- [üìà Model Performance](#-model-performance)
- [üîß Development Workflow](#-development-workflow)
- [üìö Documentation](#-documentation)
- [ü§ù Contributing](#-contributing)

---

## üéØ **Project Overview**

This project implements a **production-ready MLOps pipeline** for predicting bank marketing campaign success. The system demonstrates enterprise-grade practices including:

- **Advanced Feature Engineering** with Databricks Feature Store
- **Model Versioning** with MLflow and Unity Catalog
- **Infrastructure as Code** with modular components
- **Automated Training Pipelines** with comprehensive tracking
- **Production Deployment** patterns with monitoring

### **Business Problem**
Predict whether a client will subscribe to a bank's term deposit based on:
- **Demographics**: Age, job, marital status, education
- **Financial History**: Account balance, previous loans, defaults
- **Campaign Data**: Contact method, timing, previous outcomes

### **Technical Approach**
- **Classification Problem**: Binary prediction (subscribe: yes/no)
- **Model Types**: LightGBM with advanced preprocessing
- **Feature Engineering**: Risk scoring, campaign effectiveness, client profiling
- **Deployment**: MLflow model serving with Unity Catalog governance

---

## üèóÔ∏è **Architecture**

```mermaid
graph TB
    subgraph "üóÑÔ∏è Data Layer"
        A[Raw Data] --> B[Delta Tables]
        B --> C[Feature Tables]
        C --> D[Training Sets]
    end

    subgraph "üîß Processing Layer"
        E[DataProcessor] --> F[TableManager]
        F --> G[FeatureStoreAdapter]
        G --> H[VolumeManager]
    end

    subgraph "ü§ñ ML Pipeline"
        I[BasicModel] --> J[CustomModel]
        J --> K[FeatureEngineeringModel]
        K --> L[MLflow Tracking]
    end

    subgraph "üöÄ Deployment"
        L --> M[Unity Catalog]
        M --> N[Model Serving]
        N --> O[Monitoring]
    end

    D --> I
    style C fill:#e3f2fd
    style K fill:#e8f5e8
    style M fill:#fff3e0
```

### **Key Components**

| Component | Purpose | Technology |
|-----------|---------|------------|
| **Data Processing** | ETL and data validation | PySpark, Delta Lake |
| **Feature Engineering** | Advanced feature creation | Databricks Feature Store |
| **Model Training** | ML model development | LightGBM, scikit-learn |
| **Experiment Tracking** | Versioning and metrics | MLflow |
| **Model Registry** | Governance and deployment | Unity Catalog |
| **Infrastructure** | Resource management | TableManager, VolumeManager |

---

## üìÅ **Project Structure**

```
marvelous-databricks-course-ronaldespinozaa/
‚îú‚îÄ‚îÄ üì¶ src/                                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ bank_marketing/                        # Main package
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                       # Package initialization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py                         # üîë Configuration management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_processor.py                 # üîÑ Data preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/                           # ü§ñ Model implementations
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ basic_model.py               # Simple LightGBM model
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ custom_model.py              # Advanced pyfunc wrapper
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ feature_engineering_model.py  # Feature Store integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/                           # üõ†Ô∏è Utility functions
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ prediction_utils.py          # Prediction adjustments
‚îÇ   ‚îî‚îÄ‚îÄ infrastructure/                       # üèóÔ∏è Infrastructure components
‚îÇ       ‚îú‚îÄ‚îÄ table_manager.py                 # Unity Catalog table management
‚îÇ       ‚îú‚îÄ‚îÄ volume_manager.py                # Databricks Volume operations
‚îÇ       ‚îî‚îÄ‚îÄ feature_store_adapter.py         # Feature Store integration
‚îú‚îÄ‚îÄ üìì notebooks/                             # Databricks notebooks
‚îÇ   ‚îú‚îÄ‚îÄ week1.py                             # Environment setup
‚îÇ   ‚îú‚îÄ‚îÄ week2_train_basic_model.py           # Basic model training
‚îÇ   ‚îú‚îÄ‚îÄ week2.log_register_model.py          # Model logging demo
‚îÇ   ‚îú‚îÄ‚îÄ week2.train_register_custom_model.py # Custom model training
‚îÇ   ‚îú‚îÄ‚îÄ week3_feature_engineering_demo.py    # Feature Engineering exploration
‚îÇ   ‚îî‚îÄ‚îÄ week3_train_register_fe_model.py     # Feature Engineering model training
‚îú‚îÄ‚îÄ üîß scripts/                              # Automation scripts
‚îÇ   ‚îú‚îÄ‚îÄ 01.process_data.py                   # Data processing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ 02.train_register_model.py           # Model training pipeline
‚îÇ   ‚îî‚îÄ‚îÄ 03.train_register_fe_model.py        # Feature Engineering pipeline
‚îú‚îÄ‚îÄ üß™ tests/                                # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit_tests/                          # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/                            # Test fixtures
‚îÇ   ‚îî‚îÄ‚îÄ conftest.py                          # Test configuration
‚îú‚îÄ‚îÄ üìä data/                                 # Data storage
‚îÇ   ‚îî‚îÄ‚îÄ data.csv                             # Raw bank marketing dataset
‚îú‚îÄ‚îÄ ‚öôÔ∏è Configuration Files
‚îÇ   ‚îú‚îÄ‚îÄ project_config.yml                   # üéõÔ∏è Project settings
‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml                       # üì¶ Python packaging
‚îÇ   ‚îú‚îÄ‚îÄ databricks.yml                       # üß± Databricks bundle
‚îÇ   ‚îî‚îÄ‚îÄ .pre-commit-config.yaml              # üîç Code quality hooks
‚îú‚îÄ‚îÄ üöÄ Automation & CI/CD
‚îÇ   ‚îú‚îÄ‚îÄ Taskfile.yml                         # Task automation
‚îÇ   ‚îú‚îÄ‚îÄ init_script.sh                       # Environment setup
‚îÇ   ‚îî‚îÄ‚îÄ .github/workflows/                   # GitHub Actions
‚îî‚îÄ‚îÄ üìö Documentation
    ‚îú‚îÄ‚îÄ README.md                            # This file
    ‚îî‚îÄ‚îÄ examples/                            # Usage examples
```

---

## üöÄ **Getting Started**

### **Prerequisites**

- **Python 3.11** (matches Databricks Runtime 15.4 LTS)
- **Databricks Account** with Unity Catalog enabled
- **Git** for version control
- **UV Package Manager** (recommended) or pip

### **1. Repository Setup**

```bash
# Clone the repository
git clone https://github.com/end-to-end-mlops-databricks-3/marvelous-databricks-course-ronaldespinozaa.git
cd marvelous-databricks-course-ronaldespinozaa

# Switch to main development branch
git checkout week3
```

### **2. Local Environment Setup**

```bash
# Create virtual environment with UV (recommended)
uv venv -p 3.11 .venv
source .venv/Scripts/activate  # Linux/Mac
# .venv\Scripts\activate   # Windows

# Install dependencies
uv sync --extra dev

# Or with pip
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install
```

### **3. Databricks Configuration**

```bash
# Configure Databricks CLI
databricks configure --token
# Enter your workspace URL and personal access token

# Verify connection
databricks workspace list
```

### **4. Environment Variables**

Create a `.env` file (don't commit to git):

```bash
# Databricks Configuration
DATABRICKS_HOST=https://your-workspace.cloud.databricks.com
DATABRICKS_TOKEN=your-personal-access-token

# Environment Settings
ENVIRONMENT=dev
CATALOG_NAME=mlops_dev
SCHEMA_NAME=your_username
```

---

## ‚öôÔ∏è **Configuration**

The project uses environment-aware configuration through `project_config.yml`:

```yaml
# Environment-specific settings
dev:
  catalog_name: mlops_dev
  schema_name: espinoza
  volume_name: bank_marketing_dev

# Model parameters
parameters:
  learning_rate: 0.05
  n_estimators: 200
  max_depth: 4
  random_state: 42

# Feature definitions
num_features:
  - age
  - balance
  - duration
  - campaign

cat_features:
  - job
  - marital
  - education
  - contact

target: Target

# Feature Engineering (Week 3+)
experiment_name_fe: /Shared/bank-marketing-fe
```

### **Configuration Management**

```python
from bank_marketing.config import ProjectConfig

# Load environment-specific config
config = ProjectConfig.from_yaml("project_config.yml", env="dev")

# Access configuration
print(f"Catalog: {config.catalog_name}")
print(f"Features: {config.num_features}")
print(f"Parameters: {config.parameters}")
```

---

## üìä **Data Pipeline**

### **Dataset: Bank Marketing Campaign**

- **Source**: [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)
- **Records**: ~45,000 marketing campaign contacts
- **Features**: 17 attributes (demographics, financial, campaign)
- **Target**: Binary classification (subscribe: yes/no)

### **Data Processing Pipeline**

```bash
# Run data processing pipeline
python scripts/01.process_data.py --env dev
```

**Pipeline Steps:**
1. **Data Validation**: Schema validation and quality checks
2. **Data Cleaning**: Handle missing values and outliers
3. **Feature Engineering**: Create derived features
4. **Train/Test Split**: Stratified 70/30 split
5. **Delta Storage**: Save to Unity Catalog tables

### **Data Quality Monitoring**

```python
from bank_marketing.data_processor import DataProcessor

# Initialize processor
processor = DataProcessor(df, config, spark)

# Run quality checks
processor.validate_data()
processor.generate_data_profile()
```

---

## ü§ñ **Model Training**

The project implements **three model approaches** with increasing sophistication:

### **1. Basic Model (Week 2)**

Simple LightGBM classifier with standard preprocessing:

```bash
# Train basic model
python scripts/02.train_register_model.py \
  --env dev \
  --git_sha $(git rev-parse --short HEAD) \
  --branch week3
```

**Features:**
- ‚úÖ Standard preprocessing pipeline
- ‚úÖ MLflow experiment tracking
- ‚úÖ Model registration in Unity Catalog
- ‚úÖ Performance metrics logging

### **2. Custom Model (Week 2)**

Advanced pyfunc wrapper with custom prediction logic:

```python
class BankMarketingModelWrapper(PythonModel):
    def predict(self, context, model_input):
        # Custom preprocessing
        # Model prediction
        # Custom postprocessing
        return enhanced_predictions
```

**Features:**
- ‚úÖ Custom prediction output format
- ‚úÖ Business logic integration
- ‚úÖ External data source support
- ‚úÖ Enhanced error handling

### **3. Feature Engineering Model (Week 3)**

Enterprise-grade model with Feature Store integration:

```bash
# Train feature engineering model
python scripts/03.train_register_fe_model.py \
  --env dev \
  --git_sha $(git rev-parse --short HEAD) \
  --branch week3 \
  --include_external_features
```

**Features:**
- ‚úÖ Feature Store integration
- ‚úÖ Advanced feature functions
- ‚úÖ External data sources
- ‚úÖ Comprehensive lineage tracking

---

## üè™ **Feature Engineering**

### **Feature Store Architecture**

```mermaid
graph LR
    subgraph "üè™ Feature Store"
        A[Client Demographics] --> D[Training Set]
        B[Campaign History] --> D
        C[External Features] --> D
    end

    subgraph "‚öôÔ∏è Feature Functions"
        E[Risk Scoring] --> D
        F[Campaign Analysis] --> D
        G[Client Profiling] --> D
    end

    D --> H[Model Training]
    D --> I[Batch Scoring]

    style A fill:#e3f2fd
    style E fill:#f3e5f5
    style H fill:#e8f5e8
```

### **Feature Tables**

#### **1. Client Demographics**
```sql
CREATE TABLE client_demographics (
    client_id STRING PRIMARY KEY,
    age INT,
    job STRING,
    marital STRING,
    education STRING,
    balance BIGINT,
    risk_segment STRING  -- Computed field
);
```

#### **2. Campaign History**
```sql
CREATE TABLE campaign_history (
    client_id STRING PRIMARY KEY,
    contact_type STRING,
    call_duration INT,
    campaign_sequence INT,
    previous_outcome STRING
) PARTITIONED BY (contact_month);
```

### **Advanced Feature Functions**

#### **Account Risk Scoring**
```python
def calculate_account_risk_score(balance, default_credit, housing_loan, personal_loan, age, job):
    """Multi-factor risk assessment returning STRUCT<risk_score: DOUBLE, risk_category: STRING>"""
    # Sophisticated business logic
    return {"risk_score": computed_score, "risk_category": risk_level}
```

#### **Campaign Effectiveness Analysis**
```python
def calculate_campaign_effectiveness(call_duration, campaign_sequence, previous_outcome):
    """Campaign optimization analysis returning effectiveness metrics"""
    # Advanced campaign analysis
    return {"effectiveness_score": score, "optimal_timing": is_optimal}
```

### **Feature Engineering Demo & Training**

#### **Interactive Exploration Notebook**
Comprehensive Feature Engineering exploration and testing:

```bash
# Open Databricks notebook for exploration
notebooks/week3_feature_engineering_demo.py
```

**Demo Coverage:**
- ‚úÖ Feature table creation and population
- ‚úÖ Feature function testing and validation
- ‚úÖ Training set creation with lookups
- ‚úÖ Model training with enhanced features
- ‚úÖ Performance analysis and comparison

#### **Production Training Notebook**
Complete Feature Engineering model training pipeline:

```bash
# Open Databricks notebook for training
notebooks/week3_train_register_fe_model.py
```

**Training Pipeline:**
- ‚úÖ Infrastructure setup and validation
- ‚úÖ Feature table population and optimization
- ‚úÖ Advanced training set creation
- ‚úÖ Model training with computed features
- ‚úÖ MLflow logging with feature lineage
- ‚úÖ Model registration and batch scoring demo

---

## üìà **Model Performance**

### **Performance Metrics**

| Model Type | ROC AUC | Precision | Recall | F1 Score | Features | Notebook |
|------------|---------|-----------|--------|----------|----------|----------|
| **Basic Model** | 0.857 | 0.752 | 0.681 | 0.714 | 16 | `week2_train_basic_model.py` |
| **Custom Model** | 0.872 | 0.768 | 0.695 | 0.729 | 16 + custom logic | `week2.train_register_custom_model.py` |
| **Feature Engineering** | 0.894 | 0.823 | 0.741 | 0.780 | 25+ computed features | `week3_train_register_fe_model.py` |

### **Feature Importance**

**Top Features (Feature Engineering Model):**
1. **Duration** (0.185) - Call duration in seconds
2. **Account Risk Score** (0.142) - Computed risk assessment
3. **Balance** (0.128) - Account balance
4. **Campaign Effectiveness** (0.119) - Campaign quality score
5. **Age** (0.095) - Client age
6. **Client Profile Score** (0.087) - Demographic targeting score

### **Business Impact**

- **Campaign Efficiency**: +23% improvement in targeting precision
- **Cost Reduction**: 31% fewer unsuccessful calls
- **Revenue Impact**: Estimated +$2.3M annual improvement
- **Customer Experience**: Reduced over-contacting by 45%

---

## üîß **Development Workflow**

### **Git Workflow**

```bash
# Create feature branch
git checkout -b feature/new-model-approach

# Make changes and commit
git add .
git commit -m "Add advanced feature engineering model"

# Push and create PR
git push origin feature/new-model-approach
```

### **Code Quality**

The project enforces high code quality standards:

```bash
# Run pre-commit hooks
pre-commit run --all-files

# Run tests
pytest tests/ -v

# Code formatting
ruff format src/
ruff check src/ --fix
```

### **CI/CD Pipeline**

```yaml
# .github/workflows/ci.yml
name: MLOps CI/CD
on: [push, pull_request]
jobs:
  quality:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: uv sync --extra dev
      - name: Run tests
        run: pytest
      - name: Code quality
        run: pre-commit run --all-files
```

### **Development Environment**

```bash
# Setup development environment
task setup

# Run data pipeline
task process-data

# Train models
task train-basic
task train-custom
task train-fe

# Run tests
task test

# Code quality
task lint
task format
```

---

## üìö **Documentation**

### **API Documentation**

Generate API documentation:

```bash
# Generate docs
pdoc src/bank_marketing --html --output-dir docs/

# Serve locally
python -m http.server 8000 --directory docs/
```

### **Notebooks**

Interactive documentation through Databricks notebooks:

- **[Environment Setup](notebooks/week1.py)** - Project initialization and validation
- **[Basic Training](notebooks/week2_train_basic_model.py)** - Simple model training pipeline
- **[Custom Models](notebooks/week2.train_register_custom_model.py)** - Advanced model patterns
- **[Feature Engineering Demo](notebooks/week3_feature_engineering_demo.py)** - Interactive FE exploration
- **[Feature Engineering Training](notebooks/week3_train_register_fe_model.py)** - Complete FE model pipeline

### **Architecture Decision Records (ADRs)**

Key architectural decisions documented in `docs/adr/`:

- **ADR-001**: Unity Catalog for data governance
- **ADR-002**: LightGBM for baseline models
- **ADR-003**: Feature Store for enterprise features
- **ADR-004**: pyfunc for custom model logic

---

## ü§ù **Contributing**

### **Development Setup**

1. **Fork the repository**
2. **Create feature branch** from `main`
3. **Follow code standards** (ruff, mypy, pytest)
4. **Add comprehensive tests**
5. **Update documentation**
6. **Submit pull request**

### **Code Standards**

- **Type Hints**: All functions must have type annotations
- **Docstrings**: Google-style docstrings required
- **Testing**: Minimum 80% code coverage
- **Linting**: Pass all ruff and mypy checks

### **Contribution Guidelines**

```python
def example_function(param1: int, param2: str) -> Dict[str, Any]:
    """Example function with proper documentation.

    Args:
        param1: Integer parameter description
        param2: String parameter description

    Returns:
        Dictionary with results

    Raises:
        ValueError: If parameters are invalid
    """
    # Implementation here
    pass
```

---

## üìä **Project Status**

### **Completed Features** ‚úÖ

- [x] **Week 1**: Environment setup and infrastructure validation
- [x] **Week 2**: Basic and custom model training with MLflow
- [x] **Week 3**: Advanced Feature Engineering with Feature Store
- [x] **Infrastructure**: Production-ready TableManager and VolumeManager
- [x] **Feature Engineering**: Enterprise-grade feature tables and functions
- [x] **Model Training**: Three-tier model architecture (Basic ‚Üí Custom ‚Üí FE)
- [x] **Testing**: Comprehensive test suite with fixtures
- [x] **Documentation**: Complete API docs and interactive notebooks
- [x] **Notebooks**: Interactive exploration and production training pipelines

### **In Progress** üöß

- [ ] **Week 4**: Model serving and real-time inference
- [ ] **Week 5**: CI/CD pipeline automation
- [ ] **Week 6**: Advanced monitoring and alerting
- [ ] **Week 7**: Model governance and compliance
- [ ] **Week 8**: Production deployment strategies

### **Future Enhancements** üîÆ

- [ ] **Real-time Feature Serving**: Online feature store
- [ ] **A/B Testing Framework**: Model comparison infrastructure
- [ ] **Automated Retraining**: Drift-based model updates
- [ ] **Multi-model Ensemble**: Advanced prediction strategies
- [ ] **Explainability**: SHAP and LIME integration

---

## üìû **Support**

### **Getting Help**

- **Issues**: [GitHub Issues](https://github.com/end-to-end-mlops-databricks-3/marvelous-databricks-course-ronaldespinozaa/issues)
- **Discussions**: [GitHub Discussions](https://github.com/end-to-end-mlops-databricks-3/marvelous-databricks-course-ronaldespinozaa/discussions)
- **Documentation**: Check `docs/` directory
- **Examples**: See `examples/` for usage patterns

### **Common Issues**

**Issue**: `Table not found error`
```bash
# Solution: Ensure schema exists
python -c "
from bank_marketing.config import ProjectConfig
from infrastructure.table_manager import TableManager
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
config = ProjectConfig.from_yaml('project_config.yml', env='dev')
tm = TableManager(spark, config)
tm.ensure_schema_exists()
"
```

**Issue**: `Feature function not found`
```bash
# Solution: Run complete feature engineering setup
python scripts/03.train_register_fe_model.py --env dev --git_sha test --job_run_id test --branch week3

# Or use interactive notebook
# Upload and run: notebooks/week3_train_register_fe_model.py
```

**Issue**: `Struct field extraction errors`
```python
# Solution: Use safe extraction pattern
def extract_risk_data(x):
    if x is None or not isinstance(x, dict):
        return {"risk_score": 0.5, "risk_category": "MEDIUM"}
    return {
        "risk_score": x.get("risk_score", 0.5),
        "risk_category": x.get("risk_category", "MEDIUM")
    }
```

---

## üìú **License**

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## üôè **Acknowledgments**

- **Databricks** for providing the MLOps platform
- **MLflow** community for experiment tracking tools
- **UCI ML Repository** for the bank marketing dataset
- **End-to-End MLOps Course** instructors and community
- **Contributors** who helped improve this project

---

## üöÄ **Quick Start Commands**

```bash
# Complete setup and first run
git clone https://github.com/end-to-end-mlops-databricks-3/marvelous-databricks-course-ronaldespinozaa.git
cd marvelous-databricks-course-ronaldespinozaa
git checkout week3  # Use latest feature branch
uv venv -p 3.11 .venv && source .venv/bin/activate
uv sync --extra dev
databricks configure --token

# Run complete pipeline
python scripts/01.process_data.py --env dev
python scripts/02.train_register_model.py --env dev --git_sha $(git rev-parse --short HEAD) --branch week3
python scripts/03.train_register_fe_model.py --env dev --git_sha $(git rev-parse --short HEAD) --branch week3

# Interactive Feature Engineering (run in Databricks)
# 1. Upload notebooks/week3_feature_engineering_demo.py
# 2. Upload notebooks/week3_train_register_fe_model.py
# 3. Execute step-by-step for complete FE pipeline

# Verify results in Databricks
databricks workspace list /Shared/bank-marketing-*
```

---

**Built with ‚ù§Ô∏è for production-ready MLOps on Databricks**
