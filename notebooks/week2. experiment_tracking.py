# Databricks notebook source
# MAGIC %md
# MAGIC # MLflow Experiment Tracking - Demostraci√≥n
# MAGIC
# MAGIC Este notebook demuestra el uso de MLflow para seguimiento de experimentos con datos sint√©ticos.

# COMMAND ----------

import json
import mlflow
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime
from marvelous.common import is_databricks
from dotenv import load_dotenv


# COMMAND ----------
mlflow.get_tracking_uri()

# COMMAND ----------
if not is_databricks():
    load_dotenv()
    profile = os.environ["PROFILE"]
    mlflow.set_tracking_uri(f"databricks://{profile}")

mlflow.get_tracking_uri()
# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Configuraci√≥n de MLflow

# COMMAND ----------

# Obtener la URI de tracking actual
print(f"URI de tracking actual: {mlflow.get_tracking_uri()}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Creaci√≥n y configuraci√≥n de experimentos

# COMMAND ----------

# Crear un experimento de demostraci√≥n
experiment_name = "/Shared/bank-marketing-tracking-demo"
experiment = mlflow.set_experiment(experiment_name)

# Establecer tags para el experimento
mlflow.set_experiment_tags(
    {"project": "bank_marketing_demo", "domain": "financial_services", "purpose": "experiment_tracking_demo"}
)

print(f"Experimento configurado: {experiment.name}")
print(f"ID del experimento: {experiment.experiment_id}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Run b√°sico con par√°metros y m√©tricas

# COMMAND ----------

# Ejemplo de run b√°sico
with mlflow.start_run(
    run_name="demo-basic-tracking",
    tags={"git_sha": "demo123", "branch": "experiment-tracking", "model_type": "demo"},
    description="Demostraci√≥n b√°sica de tracking con MLflow",
) as run:
    run_id = run.info.run_id
    print(f"Run ID: {run_id}")

    # Registrar par√°metros (configuraci√≥n del modelo)
    mlflow.log_params({"learning_rate": 0.05, "n_estimators": 100, "max_depth": 5, "model_type": "lightgbm"})

    # Simular m√©tricas de entrenamiento
    metrics = {"accuracy": 0.85, "precision": 0.82, "recall": 0.78, "f1_score": 0.80, "auc": 0.88}

    # Registrar m√©tricas
    mlflow.log_metrics(metrics)

    print("‚úÖ Par√°metros y m√©tricas registrados")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Logging de m√©tricas din√°micas (√©pocas de entrenamiento)

# COMMAND ----------

# Simular entrenamiento con m√∫ltiples √©pocas
with mlflow.start_run(run_name="demo-dynamic-metrics") as run:
    # Simular mejora de m√©tricas durante el entrenamiento
    for epoch in range(10):
        # Simular m√©tricas que mejoran con el tiempo
        train_loss = 1.0 - (epoch * 0.08) + np.random.normal(0, 0.02)
        val_loss = 1.1 - (epoch * 0.07) + np.random.normal(0, 0.03)
        accuracy = 0.5 + (epoch * 0.04) + np.random.normal(0, 0.01)

        # Registrar m√©tricas con step para crear gr√°ficos temporales
        mlflow.log_metrics(
            {
                "train_loss": max(0.1, train_loss),
                "val_loss": max(0.15, val_loss),
                "accuracy": min(0.95, max(0.5, accuracy)),
            },
            step=epoch,
        )

    print("‚úÖ M√©tricas din√°micas registradas")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Logging de artefactos y visualizaciones

# COMMAND ----------

with mlflow.start_run(run_name="demo-artifacts") as run:
    # 1. Registrar texto simple
    mlflow.log_text("Experimento de demostraci√≥n para Bank Marketing", "experiment_notes.txt")

    # 2. Registrar diccionario como JSON
    model_config = {
        "architecture": "gradient_boosting",
        "features": ["age", "job", "balance", "housing"],
        "target": "subscription",
        "preprocessing": {"scaling": "standard", "encoding": "onehot"},
    }
    mlflow.log_dict(model_config, "model_config.json")

    # 3. Crear y registrar un gr√°fico
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # Gr√°fico de importancia de caracter√≠sticas (simulado)
    features = ["age", "balance", "duration", "campaign", "housing"]
    importance = [0.25, 0.30, 0.20, 0.15, 0.10]

    ax1.barh(features, importance)
    ax1.set_xlabel("Importancia")
    ax1.set_title("Importancia de Caracter√≠sticas")

    # Gr√°fico de distribuci√≥n de target (simulado)
    labels = ["No suscribe", "S√≠ suscribe"]
    sizes = [70, 30]

    ax2.pie(sizes, labels=labels, autopct="%1.1f%%")
    ax2.set_title("Distribuci√≥n del Target")

    plt.tight_layout()
    mlflow.log_figure(fig, "model_analysis.png")
    plt.close()

    # 4. Crear y registrar m√∫ltiples im√°genes
    for i in range(3):
        # Simular gr√°ficos de convergencia
        epochs = range(20)
        loss = [1.0 * (0.9**epoch) + np.random.normal(0, 0.02) for epoch in epochs]

        plt.figure(figsize=(8, 5))
        plt.plot(epochs, loss, label=f"Experiment {i + 1}")
        plt.xlabel("√âpoca")
        plt.ylabel("Loss")
        plt.title(f"Convergencia del Modelo - Experimento {i + 1}")
        plt.legend()
        plt.grid(True)

        # Guardar como archivo temporal
        temp_path = f"convergence_exp_{i}.png"
        plt.savefig(temp_path)
        plt.close()

        # Registrar en MLflow
        mlflow.log_artifact(temp_path, "convergence_plots")

        # Limpiar archivo temporal
        os.remove(temp_path)

    print("‚úÖ Artefactos y visualizaciones registrados")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Runs anidados para ajuste de hiperpar√°metros

# COMMAND ----------

# Demostraci√≥n de runs anidados
with mlflow.start_run(run_name="hyperparameter_tuning_demo") as parent_run:
    print(f"Run principal iniciado: {parent_run.info.run_id}")

    # Par√°metros del experimento
    mlflow.log_params(
        {"dataset": "bank_marketing_demo", "target": "subscription", "cv_folds": 5, "search_type": "grid_search"}
    )

    best_score = 0
    best_params = {}

    # Grid search simulado
    for learning_rate in [0.01, 0.05, 0.1]:
        for max_depth in [3, 5, 7]:
            for n_estimators in [50, 100]:
                # Crear run hijo
                with mlflow.start_run(
                    run_name=f"lr_{learning_rate}_depth_{max_depth}_est_{n_estimators}", nested=True
                ) as child_run:
                    # Registrar par√°metros espec√≠ficos
                    params = {"learning_rate": learning_rate, "max_depth": max_depth, "n_estimators": n_estimators}
                    mlflow.log_params(params)

                    # Simular entrenamiento y evaluaci√≥n
                    base_score = 0.75
                    lr_bonus = learning_rate * 2
                    depth_bonus = max_depth / 20
                    est_bonus = n_estimators / 1000

                    # A√±adir algo de ruido
                    noise = np.random.normal(0, 0.02)
                    score = min(0.95, base_score + lr_bonus + depth_bonus + est_bonus + noise)

                    # Simular tiempo de entrenamiento
                    training_time = 10 + (n_estimators / 10) + np.random.normal(0, 2)

                    # Registrar m√©tricas
                    mlflow.log_metrics({"cv_score": score, "training_time": max(1, training_time)})

                    # Tracking del mejor modelo
                    if score > best_score:
                        best_score = score
                        best_params = params

                    print(
                        f"    Configuraci√≥n: LR={learning_rate}, Depth={max_depth}, Est={n_estimators}, Score={score:.4f}"
                    )

    # Registrar el mejor resultado en el run padre
    mlflow.log_metrics({"best_cv_score": best_score})
    mlflow.log_params({f"best_{k}": v for k, v in best_params.items()})

    print(f"\n‚úÖ Mejor configuraci√≥n: {best_params} con score: {best_score:.4f}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7. B√∫squeda y an√°lisis de experimentos

# COMMAND ----------

# Buscar runs con filtros
from time import time

time_hour_ago = int(time() - 3600) * 1000

# IMPORTANT: Replace 'your_experiment_name' with the actual name of your MLflow experiment

try:
    runs = mlflow.search_runs(
        experiment_names=[experiment_name],
        order_by=["start_time DESC"],
        filter_string="status='FINISHED' AND "
        f"start_time>{time_hour_ago} AND "
        "tags.mlflow.runName LIKE '%demo%'",  # Changed to use tags.mlflow.runName
    )

    print(f"Runs encontrados en la √∫ltima hora: {len(runs)}")

    if not runs.empty:
        # Define the relevant columns you want to display
        relevant_columns = [
            "tags.mlflow.runName",
            "start_time",
            "metrics.best_cv_score",
            "params.model_type",
            "run_id",  # Adding run_id as it's always good for identification
        ]

        # Filter the DataFrame to only include the relevant columns, if they exist
        # Also, ensure 'start_time' is converted to a readable format if present
        display_runs = pd.DataFrame()
        if not runs.empty:
            # Select columns that exist in the DataFrame
            existing_relevant_columns = [col for col in relevant_columns if col in runs.columns]
            display_runs = runs[existing_relevant_columns].copy()

            # Convert 'start_time' to datetime if it exists
            if "start_time" in display_runs.columns:
                display_runs["start_time"] = pd.to_datetime(display_runs["start_time"], unit="ms")

        print("\nResumen de runs con columnas relevantes:")
        print(display_runs.head())

        # Show metrics of the best runs, using 'metrics.best_cv_score'
        if "metrics.best_cv_score" in runs.columns:
            best_runs = runs.nlargest(3, "metrics.best_cv_score")
            print("\nTop 3 runs por 'Best CV Score':")
            for idx, run in best_runs.iterrows():
                run_name = run["tags.mlflow.runName"] if "tags.mlflow.runName" in run else "N/A"
                print(f"- {run_name}: Best CV Score = {run['metrics.best_cv_score']:.4f}")
        else:
            print("\nLa m√©trica 'metrics.best_cv_score' no est√° disponible en los runs.")

    else:
        print("No se encontraron runs que coincidan con los criterios de filtro.")

except Exception as e:
    print(f"An error occurred: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8. Recuperaci√≥n de artefactos

# COMMAND ----------

# Cargar artefactos del √∫ltimo run
if not runs.empty:
    latest_run = runs.iloc[0]
    artifact_uri = latest_run["artifact_uri"]

    try:
        # Intentar cargar configuraci√≥n del modelo
        config_data = mlflow.artifacts.load_dict(f"{artifact_uri}/model_config.json")
        print("üìä Configuraci√≥n del modelo cargada:")
        for key, value in config_data.items():
            print(f"   - {key}: {value}")
    except Exception as e:
        print(f"‚ö†Ô∏è No se pudo cargar la configuraci√≥n: {e}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 9. Resumen y mejores pr√°cticas

# COMMAND ----------

print("""
‚úÖ MLflow Experiment Tracking Demo Completado

Funcionalidades demostradas:
1. ‚úÖ Configuraci√≥n de experimentos y tracking
2. ‚úÖ Logging de par√°metros, m√©tricas y artefactos
3. ‚úÖ M√©tricas din√°micas (por √©poca/iteraci√≥n)
4. ‚úÖ Visualizaciones y gr√°ficos
5. ‚úÖ Runs anidados para hiperpar√°metros
6. ‚úÖ B√∫squeda y filtrado de experimentos
7. ‚úÖ Recuperaci√≥n de artefactos

üí° Mejores pr√°cticas aprendidas:
- Usar nombres descriptivos para runs
- Incluir tags para organizaci√≥n
- Registrar tanto par√°metros como m√©tricas
- Usar runs anidados para b√∫squedas sistem√°ticas
- Guardar visualizaciones como artefactos
- Mantener experimentos organizados por proyecto

üöÄ Pr√≥ximos pasos:
- Integrar con pipeline de datos real
- Automatizar el tracking en scripts de producci√≥n
- Implementar comparaci√≥n autom√°tica de modelos
- Configurar alertas para modelos con bajo rendimiento
""")
