# Databricks notebook source
# %pip install -e ..
# %restart_python

# from pathlib import Path
# import sys
# sys.path.append(str(Path.cwd().parent / 'src'))

# COMMAND ----------
# Initialize installations

from loguru import logger
import yaml
from pyspark.sql import SparkSession
import pandas as pd

from bank_marketing.config import ProjectConfig
from bank_marketing.data_processor import DataProcessor
from infrastructure.volume_manager import VolumeManager
from marvelous.logging import setup_logging
from marvelous.timer import Timer

# COMMAND ----------
# Load configuration from YAML
config = ProjectConfig.from_yaml(config_path="../project_config.yml", env="dev")
# Setup logging
setup_logging(log_file="logs/marvelous-1.log")
logger.info("‚úÖ Configuration loaded:")
logger.info(yaml.dump(config, default_flow_style=False))

# COMMAND ----------
# Initialize Spark and load CSV from Unity Catalog
spark = SparkSession.builder.getOrCreate()
filepath = "../data/data.csv"
# Load the data
df = pd.read_csv(filepath)
logger.info(f"üì• Data loaded from: {filepath} - Shape: {df.shape}")

# COMMAND ----------
# Preprocessing
with Timer() as preprocess_timer:
    data_processor = DataProcessor(df, config, spark)
    data_processor.preprocess()
logger.info(f"‚öôÔ∏è Preprocessing completed in: {preprocess_timer}")

# COMMAND ----------
# Train/test split
X_train, X_test = data_processor.split_data()
logger.info(f"üìä Train shape: {X_train.shape} | Test shape: {X_test.shape}")

# Volume creation and verification
volume_manager = VolumeManager(spark, config)
volume_manager.ensure_volume_exists()
logger.info(f"üì¶ Volume configured: {volume_manager.volume_path}")

# COMMAND ----------
# Save to Volume
logger.info("üíæ Saving data to volume (raw + processed)")
with Timer() as volume_timer:
    data_processor.save_to_volume(X_train, X_test)
logger.info(f"‚è±Ô∏è Data saved to volume in: {volume_timer}")

# COMMAND ----------
# Save to Unity Catalog (maintained for compatibility)
try:
    logger.info("üíæ Saving train/test to Unity Catalog (raw + processed)")
    data_processor.save_to_catalog(X_train, X_test)
    # Enable Change Data Feed for all tables
    if hasattr(data_processor, "enable_change_data_feed"):
        logger.info("üîÅ Enabling Change Data Feed for tables")
        data_processor.enable_change_data_feed()
    else:
        logger.warning("‚ö†Ô∏è Method enable_change_data_feed not found")

    logger.info("‚úÖ Data available in both raw and processed formats")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Could not save to Unity Catalog: {e}")
    logger.info("‚ÑπÔ∏è Data is available in the volume")

# COMMAND ----------
# Enable Change Data Feed for volumes (optional)
try:
    logger.info("üîÅ Enabling Change Data Feed for volumes")
    data_processor.enable_volume_change_data_feed()
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Could not enable Change Data Feed for volumes: {e}")
