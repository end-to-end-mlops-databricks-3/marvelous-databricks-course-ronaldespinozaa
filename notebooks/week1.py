# Databricks notebook source
# %pip install -e ..
# %restart_python

# from pathlib import Path
# import sys
# sys.path.append(str(Path.cwd().parent / 'src'))

# COMMAND ----------
# Iniciar instalaciones 

from loguru import logger
import yaml
from pyspark.sql import SparkSession
import pandas as pd

from bank_marketing.config import ProjectConfig
from bank_marketing.data_processor import DataProcessor
# NUEVO: Importaci√≥n del VolumeManager
from infrastructure.volume_manager import VolumeManager  
from marvelous.logging import setup_logging
from marvelous.timer import Timer

# Cargar configuraci√≥n desde YAML
config = ProjectConfig.from_yaml(config_path="../project_config.yml", env="dev")

# Setup del logging
setup_logging(log_file="logs/marvelous-1.log")

logger.info("‚úÖ Configuraci√≥n cargada:")
logger.info(yaml.dump(config, default_flow_style=False))

# COMMAND ----------
# Iniciar Spark y cargar CSV desde Unity Catalog
spark = SparkSession.builder.getOrCreate()

filepath = "../data/data.csv"

# Load the data
df = pd.read_csv(filepath)

logger.info(f"üì• Datos cargados desde: {filepath} - Shape: {df.shape}")

# COMMAND ----------
# Preprocesamiento
with Timer() as preprocess_timer:
    data_processor = DataProcessor(df, config, spark)
    data_processor.preprocess()
logger.info(f"‚öôÔ∏è Preprocesamiento completado en: {preprocess_timer}")

# COMMAND ----------
# Divisi√≥n train/test
X_train, X_test = data_processor.split_data()
logger.info(f"üìä Train shape: {X_train.shape} | Test shape: {X_test.shape}")

# COMMAND ----------
# NUEVO: Creaci√≥n y verificaci√≥n del volumen

# Verificar que el volumen existe
volume_manager = VolumeManager(spark, config)
volume_manager.ensure_volume_exists()

logger.info(f"üì¶ Volumen configurado: {volume_manager.volume_path}")

# COMMAND ----------
# NUEVO: Guardar en Volumen

logger.info("üíæ Guardando datos en volumen (raw + processed)")
with Timer() as volume_timer:
    data_processor.save_to_volume(X_train, X_test)
logger.info(f"‚è±Ô∏è Datos guardados en volumen en: {volume_timer}")

# COMMAND ----------
# Guardar en Unity Catalog (mantenemos para compatibilidad)
try:
    logger.info("üíæ Guardando train/test en Unity Catalog (raw + processed)")
    data_processor.save_to_catalog(X_train, X_test)
    
    # Activar Change Data Feed para todas las tablas
    if hasattr(data_processor, 'enable_change_data_feed'):
        logger.info("üîÅ Activando Change Data Feed para tablas")
        data_processor.enable_change_data_feed()
    else:
        logger.warning("‚ö†Ô∏è M√©todo enable_change_data_feed no encontrado")
        
    logger.info("‚úÖ Datos disponibles tanto en formato raw como procesado")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è No se pudo guardar en Unity Catalog: {e}")
    logger.info("‚ÑπÔ∏è Los datos est√°n disponibles en el volumen")

# COMMAND ----------
# NUEVO: Activar Change Data Feed para vol√∫menes (opcional)

try:
    logger.info("üîÅ Activando Change Data Feed para vol√∫menes")
    data_processor.enable_volume_change_data_feed()
except Exception as e:
    logger.warning(f"‚ö†Ô∏è No se pudo activar Change Data Feed para vol√∫menes: {e}")

# COMMAND ----------